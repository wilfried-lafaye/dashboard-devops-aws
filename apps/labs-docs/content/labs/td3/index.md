---
title: "TD3 - Orchestration et Conteneurisation"
publish: true
---

# TD3 - Orchestration et Conteneurisation

## Introduction

Ce troisi√®me laboratoire marque une √©tape majeure : le passage de scripts individuels √† une v√©ritable **orchestration d'infrastructure** et √† la **conteneurisation**. Nous allons explorer comment g√©rer des applications √† l'√©chelle en utilisant des outils modernes comme Ansible (avec r√¥les avanc√©s), OpenTofu (avec modules), Docker et Kubernetes.

```mermaid
flowchart LR
    A[‚öôÔ∏è Ansible<br/>Deployment] --> B[üì¶ Packer<br/>Golden AMI]
    B --> C[üèóÔ∏è OpenTofu<br/>ASG + ALB]
    C --> D[üê≥ Docker<br/>Conteneurs]
    D --> E[‚ò∏Ô∏è Kubernetes<br/>Orchestration]
    
    style A fill:#e8f5e9
    style B fill:#e3f2fd
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#e1f5fe
```

---

## √âtape 1 : Ansible avanc√© (Boucles et Prompts)

Nous am√©liorons notre utilisation d'Ansible pour d√©ployer plusieurs instances dynamiquement.

### Playbook interactif (`create_ec2_instances_playbook.yml`)

Ce playbook demande √† l'utilisateur combien d'instances cr√©er et d√©ploie l'infrastructure correspondante.

```yaml
- name: Deploy EC2 instances in AWS
  hosts: localhost
  gather_facts: no
  vars_prompt:                                                
    - name: num_instances
      prompt: How many instances to create?
      private: false
    - name: base_name
      prompt: What to use as the base name for resources?
      private: false
  tasks:
    # Cr√©ation dynamique des instances avec une boucle
    - name: Create EC2 instances with Amazon Linux 2023 AMI
      loop: "{{ range(num_instances | int) | list }}"         
      amazon.aws.ec2_instance:
        name: "{{ '%s-%d' | format(base_name, item) }}"       
        key_name: "{{ aws_ec2_key_pair.key.name }}"
        instance_type: t3.micro
        security_group: "{{ aws_security_group.group_id }}"
        image_id: ami-0fa3fe0fa7920f68e
```

### Solutions des Exercices (Section 1)

**1. Inventaire Dynamique (`inventory.aws_ec2.yml`)**
Au lieu de lister les IP manuellement, nous utilisons le plugin `aws_ec2` pour d√©couvrir les instances via leurs tags.
```yaml
plugin: amazon.aws.aws_ec2
regions:
  - us-east-1
keyed_groups:
  - key: tags.Ansible
    prefix: ""
```

**2. Load Balancer Nginx**
Pour r√©partir la charge, nous avons configur√© une instance Nginx avec un playbook d√©di√© (`configure_nginx_playbook.yml`) qui utilise le r√¥le `nginx`.
Ce r√¥le g√©n√®re dynamiquement la configuration `nginx.conf` en listant les IPs des serveurs d'application (r√©cup√©r√©es via les "facts" Ansible).

**3. Rolling Update avec Ansible**
Pour mettre √† jour sans interruption, nous ajoutons `serial: 1` dans le playbook. Ansible mettra √† jour les instances une par une.
*V√©rification* : Pendant que le playbook tourne, lancez une boucle `while true; do curl http://<IP>; sleep 0.5; done` pour voir la r√©ponse changer sans erreur 500.

---

## √âtape 2 : Packer et Golden Images

Plut√¥t que d'installer l'application au d√©marrage, nous cr√©ons une **Golden Image** (AMI) pr√©-configur√©e optimis√©e pour la production avec `pm2`.

### Template Packer (`sample-app.pkr.hcl`)

```hcl
packer {
  required_plugins {
    amazon = {
      source  = "github.com/hashicorp/amazon"
      version = ">= 1.3.1"
    }
  }
}

source "amazon-ebs" "amazon_linux" {
  ami_name        = "sample-app-packer-${uuidv4()}"
  instance_type   = "t3.micro"
  region          = "us-east-1"
  source_ami      = "ami-0fa3fe0fa7920f68e"
  ssh_username    = "ec2-user"
}

build {
  sources = ["source.amazon-ebs.amazon_linux"]

  provisioner "shell" {
    inline = [
      "curl -fsSL https://rpm.nodesource.com/setup_21.x | sudo bash -",
      "sudo yum install -y nodejs",
      "sudo npm install pm2@latest -g",                                  
      "eval \"$(sudo su app-user bash -c 'pm2 startup' | tail -n1)\""    
    ]
  }
}
```

> [!TIP]
> **PM2** est un gestionnaire de processus de production pour Node.js. Il g√®re le red√©marrage automatique, les logs et le monitoring.

---

## √âtape 3 : OpenTofu avec Auto Scaling Group (ASG)

Nous utilisons OpenTofu pour d√©ployer une architecture haute disponibilit√© avec un **Load Balancer (ALB)** et un **Auto Scaling Group (ASG)**.

### Architecture (`main.tf`)

```hcl
module "asg" {
  source = "../../modules/asg"

  name = "sample-app-asg"                                   
  ami_id = "ami-008eba0ec68324b1d" # √Ä remplacer par votre AMI Packer
  
  min_size         = 1                                      
  max_size         = 10                                     
  desired_capacity = 3                                      

  target_group_arns = [module.alb.target_group_arn]
}

module "alb" {
  source = "../../modules/alb"
  name   = "sample-app-alb" 
}
```

Cette configuration assure que :
1. Il y a toujours au moins 1 instance (max 10, souhait√© 3)
2. Si une instance crash, l'ASG la remplace
3. L'ALB distribue le trafic entre les instances saines

3. L'ALB distribue le trafic entre les instances saines

### Solutions des Exercices (Section 2)

**1. Instance Refresh (Zero Downtime)**
Pour mettre √† jour les instances (ex: nouvelle AMI) sans couper le service, nous avons configur√© le bloc `instance_refresh` dans `main.tf` :
```hcl
instance_refresh = {
  strategy = "Rolling"
  preferences = {
    min_healthy_percentage = 50
  }
}
```
Cela force l'ASG √† remplacer les instances par vagues, en gardant au moins 50% de capacit√© saine.

**2. Workflow de Mise √† jour (App -> Packer -> Tofu)**
Le cycle complet pour une mise √† jour d'application est :
1.  Modifier le code (`app.js`).
2.  Reconstruire l'AMI (`packer build ...`).
3.  Mettre √† jour l'ID de l'AMI dans `main.tf` (ou via variable).
4.  Appliquer les changements (`tofu apply`).
5.  L'ASG d√©tecte le changement de Launch Template et lance l'Instance Refresh.

---

## √âtape 4 : Conteneurisation (Docker)

Nous passons des machines virtuelles (VM) aux conteneurs l√©gers.

### Dockerfile

```dockerfile
FROM node:21.7

WORKDIR /home/node/app
COPY app.js .
EXPOSE 8080
USER node
CMD ["node", "app.js"]
```

Avantages :
- **L√©ger** : Pas d'OS complet √† d√©marrer
- **Portable** : Fonctionne partout (local, serveur, cloud)
- **Rapide** : D√©marrage en millisecondes

---

## √âtape 5 : Orchestration Kubernetes

Pour g√©rer nos conteneurs en production, nous utilisons Kubernetes.

### D√©ploiement (`sample-app-deployment.yml`)

```yaml
apiVersion: apps/v1
kind: Deployment                  
metadata:                         
  name: sample-app-deployment
spec:
  replicas: 3                     
  selector:                       
    matchLabels:
      app: sample-app-pods
  template:                       
    metadata:                     
      labels:
        app: sample-app-pods
    spec:
      containers:                 
        - name: sample-app        
          image: sample-app:v1    
          ports:
            - containerPort: 8080 
```

> [!NOTE]
> Kubernetes g√®re automatiquement la sant√© des pods (`replicas: 3` garantit 3 instances) et permet des **Rolling Updates** sans interruption de service.

> Kubernetes g√®re automatiquement la sant√© des pods (`replicas: 3` garantit 3 instances) et permet des **Rolling Updates** sans interruption de service.

### Solutions des Exercices (Section 3)

**1. Contexte Kubernetes (Local vs Cloud)**
Si `kubectl` ne voit pas vos pods, v√©rifiez le contexte :
```bash
# Local (Docker Desktop)
kubectl config use-context docker-desktop
# Cloud (EKS)
kubectl config use-context arn:aws:eks:us-east-2:XXXX:cluster/eks-sample
```

**2. D√©ploiement sur EKS (Bonus)**
Le code pour d√©ployer un cluster EKS complet est disponible dans `scripts/tofu/live/eks-sample`.
Attention : Cela co√ªte environ $0.10/heure. N'oubliez pas de `tofu destroy` √† la fin !
*V√©rification* : `kubectl get nodes` doit afficher les n≈ìuds EC2 g√©r√©s par EKS.

---

## √âtape 6 : Serverless avec AWS Lambda

L'approche **Serverless** permet d'ex√©cuter du code sans g√©rer aucun serveur. AWS Lambda ex√©cute votre fonction uniquement lorsqu'elle est sollicit√©e.

### Code de la fonction (`src/index.js`)

```javascript
exports.handler = async (event) => {
    const response = {
        statusCode: 200,
        body: JSON.stringify('Hello from Lambda!'),
    };
    return response;
};
```

### Infrastructure (`main.tf`)

Nous utilisons OpenTofu pour d√©ployer la Lambda et une API Gateway pour la rendre accessible via HTTP.

```hcl
module "lambda" {
  source = "../../modules/lambda"

  name    = "sample-app-lambda"
  src_dir = "${path.module}/src"
  runtime = "nodejs20.x"
  handler = "index.handler"
}

module "api_gateway" {
  source = "../../modules/api-gateway"

  name         = "sample-app-api"
  function_arn = module.lambda.function_arn
}
```

### Solutions des Exercices (Section 4 - Lambda)

**1. Changer de Runtime**
Pour utiliser Python par exemple, il suffit de changer le code (`index.py`) et la variable `runtime = "python3.9"` dans la configuration OpenTofu. Le module g√©rera le packaging du nouveau code.

**2. Ajouter des routes**
L'API Gateway peut g√©rer plusieurs routes (ex: `/users`, `/products`). Dans OpenTofu, cela se configure via la variable `routes` du module API Gateway, qui cr√©era les ressources `aws_api_gateway_resource` et `aws_api_gateway_method` correspondantes.

**3. Gestion d'erreurs**
Si la fonction Lambda √©choue (ex: `throw new Error("Oups")`), l'API Gateway renverra un code 502 (Bad Gateway). Pour g√©rer les erreurs proprement, le code doit `catch` les exceptions et retourner un objet avec `statusCode: 400` ou `500` et un body JSON explicite.

---

## üî¥ Probl√®mes rencontr√©s et Solutions

Ce lab pr√©sente une complexit√© accrue. Voici les probl√®mes critiques identifi√©s et comment les r√©soudre.

### 1. Cl√© SSH Manquante (Ansible)

> [!CAUTION]
> **Erreur** : `no such identity: ansible-ch3.key: No such file or directory`

**Cause** : La cl√© existe dans AWS mais le fichier local `.key` a √©t√© perdu.
**Solution** :
1. Supprimer la cl√© AWS : `aws ec2 delete-key-pair --key-name ansible-ch3`
2. Recr√©er et sauvegarder : 
   ```bash
   aws ec2 create-key-pair --key-name ansible-ch3 --query 'KeyMaterial' --output text > ansible-ch3.key
   chmod 400 ansible-ch3.key
   ```

### 2. AMI ID Hardcod√©e (OpenTofu)

> [!WARNING]
> **Erreur** : `The image ID 'None' is not valid` lors du `tofu apply`.

**Cause** : Le script ne trouve pas l'AMI g√©n√©r√©e par Packer car le filtre de nom est incorrect ou l'AMI n'existe pas.
**Solution** :
- V√©rifiez que Packer a bien termin√© sa construction (`packer build ...`).
- Assurez-vous que l'ID de l'AMI dans `main.tf` correspond √† celle g√©n√©r√©e (ou utilisez une data source pour la r√©cup√©rer dynamiquement).

### 3. Conflit de Contexte Kubernetes (Docker vs EKS)

> [!IMPORTANT]
> **Erreur** : `kubectl` ne voit pas vos pods ou le d√©ploiement √©choue.

**Cause** : `kubectl` pointe vers le mauvais cluster (Docker Desktop local au lieu d'AWS EKS, ou inversement).
**Solution** : V√©rifiez et changez le contexte :
```bash
# Voir le contexte actuel
kubectl config current-context

# Passer √† Docker Desktop (local)
kubectl config use-context docker-desktop

# Passer √† AWS EKS (cloud)
aws eks update-kubeconfig --name <cluster-name>
```

### 4. Co√ªts AWS (EKS)

> [!CAUTION]
> **EKS n'est PAS inclus dans le Free Tier !** (~$0.10/heure par cluster).

**Action imp√©rative** : D√©truisez le cluster EKS d√®s que vous avez fini le test.
```bash
tofu destroy
```

### 5. Inventaire Dynamique Ansible

**Probl√®me** : L'inventaire ne d√©tectait pas les IP publiques des nouvelles instances.
**Solution** : Dans `inventory.aws_ec2.yml`, utiliser `dns-name` et `ip-address` au lieu de l'option obsol√®te `public-ip-address`.

---

## Conclusion

Ce TD3 nous a fait traverser l'√©volution moderne du DevOps :
1. **VMs classiques** g√©r√©es par Ansible (Scripting avanc√©)
2. **Immutable Infrastructure** avec Packer (Images fixes)
3. **Infrastructure √©lastique** avec OpenTofu (ASG/ALB)
4. **Conteneurs** avec Docker (Portabilit√©)
5. **Orchestration** avec Kubernetes (Gestion √† l'√©chelle)

Cette stack compl√®te repr√©sente l'√©tat de l'art pour d√©ployer des applications r√©silientes et scalables.
